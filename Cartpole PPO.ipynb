{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minus-ceramic",
   "metadata": {},
   "source": [
    "## Algoritmo PPO: CartPole\n",
    "#### TFG: Aprendizaje por refuerzo: fundamentos teóricos del algoritmo PPO e implementación\n",
    "#### Autor: Alberto Maurel\n",
    "\n",
    "En este Notebook se implementa el algoritmo PPO para el problema CartPole, siguiendo las indicaciones de la memoria. \n",
    "\n",
    "\n",
    "En primer lugar se crea la función `run_simulation`, que se encarga de ejecutar una simulación sobre el entorno utilizando las estimaciones de las redes neuronales `model_val` (valor) y `model_res` (política)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forty-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is an implementation of the PPO algorithm, mixing the ideas shown in the paper and on the OpenAI Spinning Up webpage:\n",
    "#  https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n",
    "import gym \n",
    "from gym import envs\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# run_simulation: Runs a simulation of the chosen environments\n",
    "# Parameters:\n",
    "#  model_val: value neural network\n",
    "#  model_policy: policy neural network\n",
    "\n",
    "# Returns:\n",
    "#  observations: list of states the simulation has gone through \n",
    "#  rewards: list of rewards obtained\n",
    "#  values: list of values predicted by the neural network for each state\n",
    "#  actions: list with the action taken at each step\n",
    "#  probabilities: list of policies predicted by the neural network for each state\n",
    "def run_simulation(model_val, model_policy):\n",
    "    observations = []\n",
    "    rewards = []\n",
    "    values = []\n",
    "    actions = []\n",
    "    probabilities = []\n",
    "    \n",
    "    #Create a fresh environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    observation = env.reset()\n",
    "    \n",
    "    it = 0\n",
    "    #Iterates until we reach the maximum number of timesteps or our pole is too tilted\n",
    "    while True:\n",
    "        observation = list(observation)\n",
    "        \n",
    "        #Uncomment if you want to render the simulation\n",
    "        #env.render()\n",
    "        \n",
    "        #Ask the neural network for the current value and policy:\n",
    "        value = model_val.predict([observation])[0]\n",
    "        policy = model_policy.predict([observation])[0]\n",
    "   \n",
    "        #Choose one of the available actions following the discrete distribution provided\n",
    "        action = np.random.choice(a=[i for i in range(0, 2)], p=policy)\n",
    "            \n",
    "        #Append the gathered information to the arrays\n",
    "        observations.append(observation)  \n",
    "        values.append(value)\n",
    "        actions.append(action)\n",
    "        probabilities.append(policy)\n",
    "        \n",
    "        #Once we have decided what action we are going to take, take the step and save the reward\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        #Check if the simulation has ended\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        it = it + 1\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    #Return all the information\n",
    "    return observations, rewards, values, actions, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-speaker",
   "metadata": {},
   "source": [
    "Tras esto, creamos las redes neuronales. La red neuronal del valor (`PPONet_Val`) es bastante sencilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "delayed-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#Neural network to predict the value for each state\n",
    "class PPONet_Val:    \n",
    "\n",
    "    # build: builds a fresh neural network\n",
    "    #  Returns:\n",
    "    #   model: model with the desired architecture\n",
    "    def build(self):\n",
    "        inputs = tf.keras.layers.Input(shape = (4,))\n",
    "        x = tf.keras.layers.Dense(64)(inputs)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "        x = tf.keras.layers.Dense(64)(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "        x = tf.keras.layers.Dense(1)(x)\n",
    "        x = tf.keras.layers.Activation('relu', name=\"value_output\")(x)\n",
    "          \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=x, name=\"ppo_nn_val\")\n",
    "        \n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(lr = 0.0001), loss=\"mean_squared_error\")\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    # load_model: loads a previously saved neural network\n",
    "    #  Parameters:\n",
    "    #   path: path to the file the model has been saved into\n",
    "    #  Returns:\n",
    "    #   model: neural network\n",
    "    def load_model(self, path):\n",
    "        model = self.build()\n",
    "        model.load_weights(path)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    # save_model: saves a trained neural network\n",
    "    #  Parameters:\n",
    "    #   model: model to be saved\n",
    "    #   path: path to the file where we want to save the model    \n",
    "    def save_model(self, model, path):\n",
    "        model.save_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-drawing",
   "metadata": {},
   "source": [
    "La red neuronal para la política (`PPONet_Policy`) es bastante más compleja, puesto que incluye la función especial de pérdida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "younger-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.1\n",
    "\n",
    "#Neural network to predict the policy for each state\n",
    "class PPONet_Policy:    \n",
    "    \n",
    "    # build: builds a fresh neural network\n",
    "    #  Returns:\n",
    "    #   model: model with the desired architecture\n",
    "    def build(self):\n",
    "        inputs = tf.keras.layers.Input(shape = (4,))\n",
    "        advantages = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\n",
    "        actions = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)\n",
    "        \n",
    "        x = tf.keras.layers.Dense(64)(inputs)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "        x = tf.keras.layers.Dense(64)(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "        x = tf.keras.layers.Dense(2)(x)\n",
    "        x = tf.keras.layers.Activation('softmax', name=\"policy_output\")(x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=x, name=\"ppo_nn_policy\")\n",
    "            \n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(lr = 0.0001), loss=custom_loss)\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    # load_model: loads a previously saved neural network\n",
    "    #  Parameters:\n",
    "    #   path: path to the file the model has been saved into\n",
    "    #  Returns:\n",
    "    #   model: neural network\n",
    "    def load_model(self, path):\n",
    "        model = self.build()\n",
    "        model.load_weights(path)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    # save_model: saves a trained neural network\n",
    "    #  Parameters:\n",
    "    #   model: model to be saved\n",
    "    #   path: path to the file where we want to save the model \n",
    "    def save_model(self, model, path):\n",
    "        model.save_weights(path)\n",
    "\n",
    "# custom_loss: Custom loss function, as described in the original paper\n",
    "# This is not a standard loss function. Usually, we provide the neural network with 2 things: the predicted labels\n",
    "#  and the original labels. However, here there are no \"original\" and \"predicted\" labels, we have just an old policy\n",
    "#  and a new one. To follow the Tensorflow criteria, y_true will be the old policy (the data we provide as input \n",
    "#  during the training) and y_pred will be the new policy (the one we get directly from the neural network as the output of \n",
    "#  the given states).\n",
    "# In addition, to calculate the loss we don't only need both policies but also the actions taken and the calculated \n",
    "#  advantage estimators. This adds extra complexity, because the loss function can only handle 2 arguments, and encapsulating\n",
    "#  the loss function into another function leads to weird Tensorflow optimizing bugs. In addition, the length of both arrays \n",
    "#  (y_true and y_pred) has to be the same.\n",
    "\n",
    "# My solution is to include the actions and advantage estimators in y_true. Imagine we have the following data:\n",
    "#  old_policy: [[0.7, 0.3], [0.65, 0.35], [0.47, 0.53]]\n",
    "#  new_policy: [[0.62, 0.38], [0.73, 0.27], [0.52, 0.48]]\n",
    "#  actions: [1,0,1]\n",
    "#  advantage estimators: [-1.5, 0.75, -0.25]\n",
    "\n",
    "# Then, our parameters would be:\n",
    "#  y_true: [[0.7, 0.3, 1, -1.5], [0.65, 0.35, 0, 0.75], [0.47, 0.53, 1, -0,25]]\n",
    "#  y_pred: [[0.62, 0.38], [0.73, 0.27], [0.52, 0.48]]\n",
    "# Although the size of the second dimension is different, there is no problem as the lenght of the first dimension remains the same\n",
    "\n",
    "#  Returns:\n",
    "#   loss: calculated loss\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \n",
    "    #Reshape the y_true array (I don't know why the shape is lost during the function call)\n",
    "    y_true = tf.reshape(y_true, (len(y_true)*4,))\n",
    "\n",
    "    #We merge the old policy, actions and advantage estimators during the function call. Now, we have to split them\n",
    "    actions_data = tf.gather(params=y_true, indices=tf.range(start=2, limit=len(y_true), delta=4))\n",
    "    actions_data = tf.cast(actions_data, dtype=tf.int32)\n",
    "    \n",
    "    advantage_estimators_data_trim = tf.gather(params=y_true, indices=tf.range(start=3, limit=len(y_true), delta=4))\n",
    "    advantage_estimators_data_trim = tf.cast(advantage_estimators_data_trim, dtype=tf.float32)\n",
    "    \n",
    "    y_true_0 = tf.gather(params=y_true, indices=tf.range(start=0, limit=len(y_true), delta=4))\n",
    "    y_true_1 = tf.gather(params=y_true, indices=tf.range(start=1, limit=len(y_true), delta=4))\n",
    "    y_true_trim = tf.stack([y_true_0, y_true_1], axis = 1)\n",
    "    y_true_trim = tf.cast(y_true_trim, dtype=tf.float32)\n",
    "\n",
    "    #We provide the neural network with the probability of taking both actions for each state. However, we just need the\n",
    "    # probability of the selected action\n",
    "    old_predictions = tf.gather(params=tf.reshape(y_true_trim, [-1]), indices=tf.math.add(tf.range(start=0, limit=2*len(y_true_trim), delta=2), actions_data))\n",
    "\n",
    "    #Same with the new policy\n",
    "    new_predictions = tf.gather(params=tf.reshape(y_pred, [-1]), indices=tf.math.add(tf.range(start=0, limit=2*len(y_pred), delta=2), actions_data))\n",
    "\n",
    "    #Now, we calculate the r_theta ratios by dividing the new probability and the old one. Instead of dividing (which might \n",
    "    # lead to precision issues), we substract the logarithms, and exponentiate again the result.\n",
    "    r_theta = tf.math.exp(tf.math.subtract(tf.math.log(new_predictions), tf.math.log(old_predictions)))\n",
    "    \n",
    "    # a and b are the two terms into the minimum on the loss function, as explained in the original paper. \n",
    "    a = tf.math.multiply(r_theta, advantage_estimators_data_trim, name = 'a')\n",
    "    \n",
    "    b = tf.math.multiply(tf.clip_by_value(r_theta, 1-eps, 1+eps), advantage_estimators_data_trim, name = 'b')\n",
    "    \n",
    "    #Select the minimum\n",
    "    min_elems = tf.math.minimum(a, b, name = 'min')\n",
    "    \n",
    "    #Add all the minimums\n",
    "    clip = tf.math.reduce_sum(min_elems, name='clip')\n",
    "    \n",
    "    #We have to change the sign. The original loss function maximizes the loss function, while Tensorflow always minimize the\n",
    "    # loss. Due to that, we minimize the opposite of the original loss function, which equals to maximize the original one.\n",
    "    clip2 = tf.math.multiply(clip, -1)\n",
    "    \n",
    "    return clip2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-harvey",
   "metadata": {},
   "source": [
    "Tras esto, solo queda unir todas las partes del algoritmo. En primer lugar se definen los hiperparámetros, de acuerdo a lo explicado en la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "composite-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "\n",
    "#Number of iterations of the PPO algorithm\n",
    "NUM_ITERATIONS = 75\n",
    "\n",
    "#Simulations run each iteration\n",
    "NUM_ACTORS = 10\n",
    "\n",
    "#Discount factor\n",
    "gamma = 0.98\n",
    "\n",
    "#Number of steps the estimator will look ahead\n",
    "horizon = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-antenna",
   "metadata": {},
   "source": [
    "Para poder ver la evolución de la política y el valor, vamos a mostrarlas cada paso siguiendo la convención especificada en la sección 5.1. Para ello se definen las funciones `plot_rewards` y `plot_policy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "oriented-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#This one shows, for each state, the expected reward. On the Y axis, we have the pole angular speed (>0 numbers mean the\n",
    "# pole is moving towards the right side and <0 numbers mean the pole is moving towards the left). On the X axis, we have \n",
    "# the tilt of the pole.\n",
    "#Those states where the simulation is closer to it's end (for example, if the pole is tilted to the right and it's also \n",
    "# moving to the right) should have low value predictions. That's why a well trained model should predict high values on \n",
    "# the diagonal that goes from the top left corner to the lower right corner, and those values decrease towards the other corners\n",
    "\n",
    "# plot_rewards: plots the expected rewards for each state\n",
    "#  Parameters:\n",
    "#   current_model: trained PPONet_Val\n",
    "def plot_rewards(current_model):\n",
    "    mat = []\n",
    "    for j in np.arange(1.0,-1.0,-0.2):\n",
    "        fil = []\n",
    "        for i in np.arange(-0.40,0.40,0.1):\n",
    "            fil.append(current_model.predict([[0,0,i,j]])[0][0])\n",
    "        mat.append(fil)\n",
    "\n",
    "    a = np.array(mat)\n",
    "\n",
    "    plt.imshow(a, cmap='Blues', interpolation='nearest', extent=[-0.4,0.4,-1,1], aspect='auto')\n",
    "    plt.colorbar()\n",
    "    #plt.savefig('graficas/cartpole_value_75.png') #To save the predictions\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pediatric-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# plot_policy: plots the probability of going to the left for each state\n",
    "#  Parameters:\n",
    "#   current_model: trained PPONet_Policy\n",
    "def plot_policy(current_model):\n",
    "    mat = []\n",
    "    for j in np.arange(1.0,-1.0,-0.2):\n",
    "        fil = []\n",
    "        for i in np.arange(-0.40,0.40,0.1):\n",
    "            fil.append(current_model.predict([[0,0,i,j]])[0][0])\n",
    "        mat.append(fil)\n",
    "\n",
    "    a = np.array(mat)\n",
    "\n",
    "    plt.imshow(a, cmap='Blues', interpolation='nearest', extent=[-0.4,0.4,-1,1], aspect='auto')\n",
    "    plt.colorbar()\n",
    "    #plt.savefig('graficas/cartpole_value_75.png') #To save the predictions\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-harvest",
   "metadata": {},
   "source": [
    "Por último, este código es el encargado de realizar las simulaciones, calcular los estimadores de ventaja y las recompensas y entrenar los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "through-nylon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ppo_nn_policy\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "policy_output (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 4,610\n",
      "Trainable params: 4,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"ppo_nn_policy\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "policy_output (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 4,610\n",
      "Trainable params: 4,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "average_reward:  500.0\n",
      "it:  38\n",
      "average_reward:  500.0\n",
      "it:  39\n",
      "average_reward:  500.0\n",
      "it:  40\n",
      "average_reward:  442.5\n",
      "it:  41\n",
      "average_reward:  450.9\n",
      "it:  42\n",
      "average_reward:  459.4\n",
      "it:  43\n",
      "average_reward:  466.8\n",
      "it:  44\n",
      "average_reward:  500.0\n",
      "it:  45\n",
      "average_reward:  500.0\n",
      "it:  46\n",
      "average_reward:  417.6\n",
      "it:  47\n",
      "average_reward:  415.9\n",
      "it:  48\n",
      "average_reward:  481.5\n",
      "it:  49\n",
      "average_reward:  478.3\n",
      "it:  50\n",
      "average_reward:  500.0\n",
      "it:  51\n",
      "average_reward:  500.0\n",
      "it:  52\n",
      "average_reward:  500.0\n",
      "it:  53\n",
      "average_reward:  500.0\n",
      "it:  54\n",
      "average_reward:  500.0\n",
      "it:  55\n",
      "average_reward:  500.0\n",
      "it:  56\n",
      "average_reward:  500.0\n",
      "it:  57\n",
      "average_reward:  500.0\n",
      "it:  58\n",
      "average_reward:  500.0\n",
      "it:  59\n",
      "average_reward:  500.0\n",
      "it:  60\n",
      "average_reward:  500.0\n",
      "it:  61\n",
      "average_reward:  500.0\n",
      "it:  62\n",
      "average_reward:  500.0\n",
      "it:  63\n",
      "average_reward:  500.0\n",
      "it:  64\n",
      "average_reward:  474.9\n",
      "it:  65\n",
      "average_reward:  500.0\n",
      "it:  66\n",
      "average_reward:  500.0\n",
      "it:  67\n",
      "average_reward:  492.7\n",
      "it:  68\n",
      "average_reward:  500.0\n",
      "it:  69\n",
      "average_reward:  500.0\n",
      "it:  70\n",
      "average_reward:  448.9\n",
      "it:  71\n",
      "average_reward:  500.0\n",
      "it:  72\n",
      "average_reward:  488.2\n",
      "it:  73\n",
      "average_reward:  500.0\n",
      "it:  74\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "# Load/Create value network\n",
    "neural_network = PPONet_Val()\n",
    "model_val = neural_network.build()\n",
    "# We can also load a partially trained model\n",
    "#model_val = neural_network.load_model('cartpole_weights/val_it_37_eps_01_v2.h5')\n",
    "\n",
    "# Load/Create policy network\n",
    "neural_network2 = PPONet_Policy()\n",
    "model_pol = neural_network2.build()\n",
    "#model_pol = neural_network2.load_model('cartpole_weights/pol_it_37_eps_01_v2.h5')\n",
    "\n",
    "\n",
    "#Iterate the desired amount of iterations\n",
    "for iter in range(0, NUM_ITERATIONS):\n",
    "    observations_data = []\n",
    "    rewards_data = []\n",
    "    probabilities_data = []\n",
    "    advantage_estimators_data = []\n",
    "    actions_data = []\n",
    "    values_obtained_data = []\n",
    "    \n",
    "    #Gather all the rollouts\n",
    "    for r in range(0, NUM_ACTORS):\n",
    "        #Run one simulation\n",
    "        observations, rewards, values, actions, probabilities = run_simulation(model_val, model_pol)\n",
    "        advantage_estimators = []\n",
    "        values_obtained = []\n",
    "        \n",
    "        #Calculate the rewards\n",
    "        for i in range(0, len(rewards)):\n",
    "            val = 0\n",
    "            for j in range(i, len(rewards)):\n",
    "                val = val + gamma**(j-i)*rewards[j]\n",
    "    \n",
    "            values_obtained.append(val)\n",
    "        \n",
    "        #Calculamos los advantage estimators\n",
    "        for i in range(0,len(rewards)):\n",
    "            adv = -values[i]\n",
    "            if i + horizon < len(rewards):\n",
    "                adv = adv + gamma**horizon*values[i+horizon]\n",
    "            \n",
    "            for j in range(0, horizon):\n",
    "                if i + j == len(rewards):\n",
    "                    break\n",
    "                adv = adv + gamma**j*rewards[i+j]\n",
    "                \n",
    "            advantage_estimators.append(adv)\n",
    "            \n",
    "        observations_data.extend(copy.deepcopy(observations))\n",
    "        rewards_data.extend(copy.deepcopy(rewards))\n",
    "        values_obtained_data.extend(copy.deepcopy(values_obtained))\n",
    "        actions_data.extend(copy.deepcopy(actions))\n",
    "        probabilities_data.extend(copy.deepcopy(probabilities))\n",
    "        advantage_estimators_data.extend(copy.deepcopy(advantage_estimators))\n",
    "        \n",
    "        \n",
    "    #Update the policy network\n",
    "    print('average_reward: ', len(rewards_data)/NUM_ACTORS)\n",
    "    advantage_estimators_data = np.reshape(np.array(advantage_estimators_data), (-1))\n",
    "    y_true_np = np.array([np.concatenate([a, [b], [c]]) for a, b, c in zip(probabilities_data, actions_data, advantage_estimators_data)]) \n",
    "    model_pol.fit(x=np.array(observations_data), y=y_true_np, batch_size = 8, epochs = 5, verbose=0)\n",
    "    print('it: ', iter)\n",
    "    \n",
    "    #Update the value network\n",
    "    model_val.fit(x=np.array(observations_data), y=np.array(values_obtained_data), batch_size = 8, epochs = 5, verbose=0)\n",
    "    \n",
    "    #Uncomment to see the evolution of the policy/value\n",
    "    #plot_policy(model_pol)\n",
    "    #plot_rewards(model_val)\n",
    "    \n",
    "    #Save the trained neural networks\n",
    "    neural_network.save_model(model_val, 'cartpole_weights/val_it_' + str(iter) + '_eps_01_v2.h5')\n",
    "    neural_network2.save_model(model_pol, 'cartpole_weights/pol_it_' + str(iter) + '_eps_01_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-adelaide",
   "metadata": {},
   "source": [
    "Una vez que hemos entrenado el modelo, podemos utilizarlo para pasarnos el entorno modificando la política probabilista por una política determinista, en la que en todo momento se ejecuta la acción con mayor probabilidad. Para ello sustituimos la función `run_simulation` por `run_game`, donde la única diferencia está en cómo se escoge la acción a realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eleven-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(model_policy):\n",
    "    rewards = []\n",
    "    \n",
    "    #Create a fresh environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    observation = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        observation = list(observation)\n",
    "        \n",
    "        #Uncomment if you want to render the simulation\n",
    "        env.render()\n",
    "        \n",
    "        policy = model_policy.predict([observation])[0]\n",
    "   \n",
    "        #Choose one of the available actions following the discrete distribution provided\n",
    "        if policy[0] > 0.5:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1\n",
    "        \n",
    "        #Once we have decided what action we are going to take, take the step and save the reward\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        #Check if the simulation has ended\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print('Final reward: ', len(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lonely-cleaners",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ppo_nn_policy\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "policy_output (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 4,610\n",
      "Trainable params: 4,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD8CAYAAABq6S8VAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAa+UlEQVR4nO3df7BfdX3n8efr3iT8DL8MiSGJJdVUGztCMQJd/AFC3CTbNrLjtqEWqdXJsIVOte0u2ekO6053Z6iMresUzdzSDDitMG4FSTUakdamLaK50BgSAiREK9dkyIII8pubvPePc679+uV7v+fcnHO/53Murwdz5p7f531/5P398DmfH4oIzMysvYaaDsDMzKpxIjczazkncjOzlnMiNzNrOSdyM7OWcyI3M2u5WhK5pE2SDknaNclxSfqUpH2Sdko6p+PYKkkP5cc21BGPmdmrSV0l8puAVX2OrwaW5ct64DMAkoaBG/Ljy4HLJC2vKSYzs1eFWhJ5RGwDftjnlLXAZyNzD3CKpIXAucC+iNgfES8Bt+bnmplZSbMG9JxFwKMd22P5vl77z+t1A0nryUrzHHvc8W9dtPQN0xNpScfOGm70+ROGh9R0CAyp+RgAEgkjCf5R/Jv77rv38Yg4vco9hk/6mYjx5wvPi+f/39aI6Fc7MS0Glch7/V1Fn/2v3BkxAowAvOHNZ8UnbtlaX3RH4U3zT2r0+RNOOX520yFwzKw03pnPGm4+faXwwQogf6r9xHGz9a9V7xHjz3PMG3+t8LwXdtwwr+qzjsagEvkYsKRjezFwAJgzyX4zs4QIlEaBpZdBRbYZ+EDeeuV84KmIOAhsB5ZJWippDrAuP9fMLB0ChoaLl4bUUiKXdAtwITBP0hjwP4DZABGxEdgCrAH2Ac8BH8yPjUu6GtgKDAObImJ3HTGZmdUq4eqqWhJ5RFxWcDyAqyY5toUs0ZuZJSrtqpVB1ZGbmbXbTC+Rm5nNaMIlcjOzdpNL5GZmrddgq5QiTuRmZoX8stPMrN2Eq1bMzFrPJXIzszZz1YqZWbsJGPbLTjOzdnMduZlZm7lqxcys/VwiNzNrOZfIzcxaTO6ib2bWfu6ib2bWZn7ZWbsfv3iYb3zvR43GMO+4Yxp9/oQ5CUx8nMh8wyiBf2ipTHo81HsO84FK5WdRm4S/n1YmcjOzgUp8PPJaIpO0StJDkvZJ2tDj+H+RtCNfdkk6LOm0/Nj3JN2fHxutIx4zs3rlVStFS0Mql8glDQM3ACuBMWC7pM0R8cDEORFxPXB9fv6vAB+NiB923OaiiHi8aixmZtMm4ZeddXyEnAvsi4j9EfEScCuwts/5lwG31PBcM7PBmWiC2G9pSB2JfBHwaMf2WL7vFSQdD6wCvtCxO4CvSbpX0voa4jEzq5dmeNUK2WuAbpO9Mv8V4J+7qlUuiIgDkuYDd0p6MCK2veIhWZJfDzD39DOqxmxmNjUJt1qp4yNkDFjSsb0YODDJuevoqlaJiAP510PA7WRVNa8QESMRsSIiVhx30qmVgzYzmwpJhUtT6kjk24FlkpZKmkOWrDd3nyTpZOBdwB0d+06QNHdiHXgPsKuGmMzMapPN9JZuIq9ctRIR45KuBrYCw8CmiNgt6cr8+Mb81EuBr0XEsx2XLwBuz38As4DPRcRXq8ZkZlYrCaXS862HWjoERcQWYEvXvo1d2zcBN3Xt2w+cVUcMZmbTKeWequ7ZaWZWghO5mVnLOZGbmbWZ6N3QOhFO5GZmBUSzrVKKOJGbmZUwNJTu6IdO5GZmJaRcIk/3I8bMLBUquZS5VfGw3ydL+ltJ35G0W9IHi+7pRG5mVkIdPTs7hv1eDSwHLpO0vOu0q4AHIuIs4ELgE3mv+Uk5kZuZFZh42VlDF/0yw34HMFfZDU8EfgiM97up68jNzEoo2UV/XtdMZyMRMdKx3WvY7/O67vHnZONVHQDmAr8eEUf6PdSJ3MysiEq/7Hw8Ilb0v9MrdA/7/e+BHcC7gdeTDe/9jxHx9GQ3bWUif+7FcUb3Njsz3BknzW70+RMumXN60yEwPHRM0yEAMJRAq4IhNT97PZQuPU5vDM2HUKuaWq2UGfb7g8B1ERHAPknfBd4EfHuym7qO3MyshJrqyMsM+/194OL8mQuANwL7+920lSVyM7NBqqtnZ8lhv/8YuEnS/WRVMdcUTU7vRG5mVkZNVUVFw37ns6a9Zyr3dCI3Mysid9E3M2u9lLvoO5GbmZWRbh53IjczKyPlEnktlT4lBoG5UNJTknbky7VlrzUza1qZpodNJvrKJfKOQWBWkjV23y5pc0Q80HXqP0bELx/ltWZmjZrpJfIyg8BMx7VmZgOjIRUuTakjkfcaBGZRj/N+KR9f9yuS3jzFa5G0XtKopNGXn/1RDWGbmZU3o6tWKDcIzH3Az0TEM5LWAF8ElpW8NtuZjSA2AjB3yZvSGNDCzF4dyg+a1Yg6SuSFg8BExNMR8Uy+vgWYLWlemWvNzJomskHAipam1JHICweBkfTafJB0JJ2bP/eJMteamTVvhrdaKTkIzPuA/yxpHHgeWJcP0djz2qoxmZnVbSiBoYEnU0uHoBKDwPw52awXpa41M0tKw1UnRdyz08ysgHgVlMjNzGY6l8jNzFou5eaHTuRmZkVcR16/F18Y55G9hxqNYesxw40+f8KCE+c0HQJvnXVq0yEAMCuBOsxU6lFTSDqJzENdCyFPLGFm1nYpfDhOxonczKwE15GbmbWZ68jNzNotG2sl3UzuRG5mVkLCedyJ3MysjFRaJPXiRG5mViTx8cidyM3MCkyMR54qJ3Izs0LNjjdexInczKyEhPO4E7mZWSH5ZaeZWau5HbmZ2QyQciKvZTgvSaskPSRpn6QNPY6/X9LOfLlb0lkdx74n6X5JOySN1hGPmVndpOKlKZVL5JKGgRuAlcAYsF3S5oh4oOO07wLviognJa0GRoDzOo5fFBGPV43FzGy6zPQS+bnAvojYHxEvAbcCaztPiIi7I+LJfPMeYHENzzUzG4wSpfEm83wdiXwR8GjH9li+bzIfAr7SsR3A1yTdK2n9ZBdJWi9pVNLokReeqhSwmdlUZBNLFC9NqSOR94q+59wgki4iS+TXdOy+ICLOAVYDV0l6Z69rI2IkIlZExIqhY0+uGrOZ2ZQMSYVLGUXvFPNzLszfG+6W9A9F96yj1coYsKRjezFwoEdgbwFuBFZHxBMT+yPiQP71kKTbyapqttUQl5lZbeqoOinzTlHSKcCngVUR8X1J84vuW0eJfDuwTNJSSXOAdcDmruBfB9wGXB4RD3fsP0HS3Il14D3ArhpiMjOrjfJBs4qWEgrfKQK/AdwWEd+HrJBbdNPKJfKIGJd0NbAVGAY2RcRuSVfmxzcC1wKvAT6df7PjEbECWADcnu+bBXwuIr5aNSYzs7qVrAKf19WMeiQiRjq2e71T7GzBB/BzwGxJ3wDmAv8nIj7b76G1dAiKiC3Alq59GzvWPwx8uMd1+4GzuvcXGX/5ZZ78wWNHEWl9drzwUqPPn3D4cPNTlZ/y7tlNhwDAz886qekQGE6kG/eQmp/xPeXmekej5MvMx/NC6mTKvFOcBbwVuBg4DvimpHs6azO6uWenmVkBkbVcqUGZd4pjZB8IzwLPStpGVuCdNJE3/7FtZtYCQypeSih8pwjcAbxD0ixJx5NVvezpd1OXyM3MipR/mdlXmXeKEbFH0leBncAR4MaI6NsIxInczKyEuqr8i94p5tvXA9eXvacTuZlZAUHpDj9NcCI3MyvBE0uYmbVY04NiFXEiNzMrwVUrZmYtl24adyI3Mysl5Z6qTuRmZgWyVitNRzE5J3IzsyJqduKIIk7kZmYluGrFzKzFXLViZjYDuERuZtZy6aZxJ3Izs0JSOpOG9OJEbmZWQspVK7VMLCFplaSHJO2TtKHHcUn6VH58p6Rzyl5rZpaCifFW+i1NqZzIJQ0DNwCrgeXAZZKWd522GliWL+uBz0zhWjOzRgkxpOKlKXWUyM8F9kXE/oh4CbgVWNt1zlrgs5G5BzhF0sKS15qZNatEabzJEnkddeSLgEc7tsfI5pgrOmdRyWsBkLSerDQPx5wMzzxRKeiqnh1/sdHnT9j+zy80HQIfe6r5GAA+dmnz/zP35gUnNx0CAMNDs5sOgSF1Tw7fbinXkdeRyHt9d92/wcnOKXNttjNiBBgBGJq7aGb9hZhZ0gQMz/BEPgYs6dheDBwoec6cEteamTUu4daHtdSRbweWSVoqaQ6wDtjcdc5m4AN565Xzgaci4mDJa83MGjek4qUplUvkETEu6WpgKzAMbIqI3ZKuzI9vJJsxeg2wD3gO+GC/a6vGZGZWp+xlZrpF8lo6BEXEFrJk3blvY8d6AFeVvdbMLDUpV624Z6eZWQkJF8idyM3MigiYlXAmdyI3Mysh4TzuRG5mVkQNd8Ev4kRuZlZCwnncidzMrAy3WjEzazHhiSXMzNqt4Z6bRZzIzcxKUMKzdjqRm5kVEC6Rm5m1nhO5mVnLpTxoVi2TL5uZzWQSDA8VL+XuVW7CeUlvk3RY0vuK7ukSuZlZCXX07OyYcH4l2YQ72yVtjogHepz3J2RDfBfHVjkyM7MZbuJlZw0TS5SdcP53gS8Ah8rc1InczKyEbHKJ/gswT9Jox7K+6zaTTUTf8RwtAi4FNlJSO6tW4gi88EyzMRx+udnn5w6Pv9R0COy594WmQwDgo081H8e1636h6RAAeMeZpzcdAicfP7vpEGokhsq1I388Ilb0vdErdU8m/0ngmog4XPYFazsTuZnZAInaBs0qM1n9CuDWPInPA9ZIGo+IL05200pVK5JOk3SnpL3511N7nLNE0t9L2iNpt6Tf6zj2MUk/kLQjX9ZUicfMbFoIZg2pcCmhcML5iFgaEWdGxJnA3wC/0y+JQ/U68g3AXRGxDLgr3+42DvxBRPw8cD5wlaTlHcf/LCLOzhfP3WlmyZkokZeoI+8rIsaBiQnn9wCfn5isfmLC+qNRtWplLXBhvn4z8A3gms4TIuIgcDBf/7GkPWSV+z/V3MbMLGV1TSxRNFl91/7fKnPPqiXyBXminkjY8/udLOlM4BeBb3XsvlrSTkmbelXNdFy7fuJNcLz8XMWwzcympo4S+XQpTOSSvi5pV4+lV9vHfvc5kaxd5Eci4ul892eA1wNnk5XaPzHZ9RExEhErImKFZh8/lUebmVUismRZtDSlsGolIi6Z7JikxyQtjIiDkhYySeN1SbPJkvhfR8RtHfd+rOOcvwC+NJXgzcwGQvVVrUyHqh8im4Er8vUrgDu6T1DWhuYvgT0R8addxxZ2bF4K7KoYj5lZ7bKenSpcmlI1kV8HrJS0l2zsgOsAJJ0haaIy/wLgcuDdPZoZflzS/ZJ2AhcBH60Yj5nZtFCJpSmVWq1ExBPAxT32HwDW5Ov/xCTfY0RcXuX5ZmaDknDNint2mpkVU9LjkTuRm5kVmGi1kioncjOzElJuteJEbmZWRGlP9eZEbmZWwFUrZmYzgEvkZmYtl24adyI3MyskYNglcjOzdks4jzuRm5kVE0q4csWJ3MysBJfI6xZHoOnZ4+NIs8+fcORw0xHA4fGmIwDgwMPdk5EP3v/6fBr/2v/7r7256RB4x5mnNx1CbbLmh2n8bntpZyI3MxukhmcAKuJEbmZWgrvom5m1WDaxRNNRTM6J3MysBLdaMTNruYRrVpzIzczKSLlEXmlAL0mnSbpT0t7866mTnPe9fG7OHZJGp3q9mVmTJurIi5amVB2ZcQNwV0QsA+7KtydzUUScHRErjvJ6M7NmSAyVWJpSNZGvBW7O128G3jvg683MBkIllqZUTeQLIuIgQP51/iTnBfA1SfdKWn8U1yNpvaRRSaMx/nzFsM3MysuqVtItkRe+7JT0deC1PQ790RSec0FEHJA0H7hT0oMRsW0K1xMRI8AIwNAJC5rvi21mryrpvuoskcgj4pLJjkl6TNLCiDgoaSFwaJJ7HMi/HpJ0O3AusA0odb2ZWeMSzuRVq1Y2A1fk61cAd3SfIOkESXMn1oH3ALvKXm9mloKUq1aqJvLrgJWS9gIr820knSFpS37OAuCfJH0H+Dbw5Yj4ar/rzcxSk/LLzkodgiLiCeDiHvsPAGvy9f3AWVO53swsOQlXrbhnp5lZgazEnW4mdyI3MyuS+HjkVevIzcxeFeqqI5e0StJDkvZJekVvdknvl7QzX+6W1LNqupNL5GZmhYRqKJJLGgZuIGvcMQZsl7Q5Ih7oOO27wLsi4klJq8n6z5zX774ukZuZlSAVLyWcC+yLiP0R8RJwK9lQJT8REXdHxJP55j3A4qKbtrNEHgGHX244hkQmX44EOrmmEAMk8TsZ2zvcdAgAXH9H8/+0h96bcKXyFE2h6mRe5wivwEjeK33CIuDRju0x+pe2PwR8peihzf+2zczaoFwmf7xrhNcyd+lZEpJ0EVkif3vRQ53IzcxKqKn54RiwpGN7MXDgFc+S3gLcCKzO+9v05TpyM7MSaqoj3w4sk7RU0hxgHdlQJR3P0euA24DLI+LhMjd1idzMrEhN7cgjYlzS1cBWYBjYFBG7JV2ZH98IXAu8Bvh03lJmvKC6xonczKyMunp2RsQWYEvXvo0d6x8GPjyVezqRm5kVEGn37HQiNzMrIeE87kRuZlZKwpncidzMrIQmJ44o4kRuZlZCumncidzMrJyEM7kTuZlZgdQnlqjUs1PSaZLulLQ3/3pqj3PeKGlHx/K0pI/kxz4m6Qcdx9ZUicfMbFqU6NXZZBV61S76G4C7ImIZcFe+/VMi4qGIODsizgbeCjwH3N5xyp9NHM8bypuZJSflyZerJvK1wM35+s3AewvOvxh4JCL+teJzzcwGKJtYomhpStVEviAiDgLkX+cXnL8OuKVr39X5lEabelXNmJmloNVVK5K+LmlXj2Vt0bVd95kD/Crwfzt2fwZ4PXA2cBD4RJ/r10salTQa489P5dFmZpWUqVZpsmqlsNVKRFwy2TFJj0laGBEHJS0EDvW51Wrgvoh4rOPeP1mX9BfAl/rEMUI2dx1Dx89PZEoaM3vVSLfRSuWqlc3AFfn6FcAdfc69jK5qlTz5T7gU2FUxHjOzaaES/zWlaiK/DlgpaS/ZrNDXAUg6Q9JPWqBIOj4/flvX9R+XdL+kncBFwEcrxmNmNi1SriOv1CEon4Lo4h77DwBrOrafIxsovfu8y6s838xsIARDCVettLdnZ9Mztx853OzzJzT9c4AkZq8H0ojjqX6viQZn/4PN/9P+1NbZTYdQs3QzefO/bTOzxHliCTOzGSDhPO5EbmZWhkvkZmYt12QX/CJO5GZmJaSbxp3IzcwKNd1OvIgTuZlZCSlPLOFEbmZWRrp53InczKyMhPO4E7mZWTExlHAluRO5mVmB1Ht2Vh390MzMGuYSuZlZCSmXyJ3IzcxKcPNDM7M2c4cgM7N2S/1lpxO5mVkJrloxM2u5lEvklZofSvpPknZLOiJpRZ/zVkl6SNI+SRs69p8m6U5Je/Ovp1aJx8xsuqjE0pSq7ch3Af8R2DbZCZKGgRuA1cBy4DJJy/PDG4C7ImIZcFe+bWaWnoQzeaVEHhF7IuKhgtPOBfZFxP6IeAm4FVibH1sL3Jyv3wy8t0o8ZmbTQcCQVLg0ZRB15IuARzu2x4Dz8vUFEXEQICIOSpo/2U0krQfW55svvrDjhl3TEewUzAMebzgGSCOOFGKANOJIIQZIII7RLzQfQ+6NVW9w3333bj1utuaVOLWR77cwkUv6OvDaHof+KCLuKPGMXh9TUeK6n74gYgQYyWMajYhJ6+QHIYUYUokjhRhSiSOFGFKJI4UYJuKoeo+IWFVHLNOlMJFHxCUVnzEGLOnYXgwcyNcfk7QwL40vBA5VfJaZ2avOIAbN2g4sk7RU0hxgHbA5P7YZuCJfvwIoU8I3M7MOVZsfXippDPgl4MuStub7z5C0BSAixoGrga3AHuDzEbE7v8V1wEpJe4GV+XYZI1XirkkKMUAacaQQA6QRRwoxQBpxpBADpBPHtFHElKurzcwsIR6P3Mys5ZzIzcxarhWJfCpd+SUNS/oXSV8adAySjpX0bUnfyYcu+J91xjCFOJZI+ntJe/I4fm/QMeTnbZJ0SFJtbf4nG+6h47gkfSo/vlPSOXU9e4pxvEnSNyW9KOkPG4rh/fnPYKekuyWd1VAca/MYdkgalfT2QcfQcd7bJB2W9L66Y2hURCS/AB8HNuTrG4A/6XPu7wOfA7406BjI2syfmK/PBr4FnN9AHAuBc/L1ucDDwPJB/z6AdwLnALtqeu4w8Ajws8Ac4Dvd3xewBvhK/rs4H/jWNPw9loljPvA24H8Df9hQDP8OODVfX93gz+JE/u193FuABwcdQ8d5fwdsAd5X98+iyaUVJXJKduWXtBj4D8CNTcQQmWfyzdn5Uvfb5DJxHIyI+/L1H5O1Flo0yBjyZ28Dfljjc/sN99AZ22fz38U9wCl5H4U6FcYREYciYjvwcs3PnkoMd0fEk/nmPWR9OJqI45nIMylwAvX/myjzdwHwu8AXmIH9VdqSyH+qKz9ZaaeXTwL/FTjSVAx51c4Osj+WOyPiW03E0RHPmcAvkv3fQSMx1KjXcA/dH1BlzhlEHNNtqjF8iOz/VBqJI2+q/CDwZeC3Bx2DpEXApcDGmp+dhGTGI+83FEDJ638ZOBQR90q6sIkYACLiMHC2pFOA2yX9QkRMqY64jjjy+5xIVgL5SEQ83UQMNSsz3EMtQ0LUEMd0Kx2DpIvIEnntddNl44iI28n+PbwT+GOgao/xqcbwSeCaiDislAcWP0rJJPLoMxSApDJd+S8AflXSGuBY4CRJfxURvznAGDrv9SNJ3wBWkQ33W1odcUiaTZbE/zoibpvK8+uKYRr0G+5hKucMIo7pVioGSW8hq2pcHRFPNBXHhIjYJun1kuZFRF0DTJWJYQVwa57E5wFrJI1HxBdriqFRbalaKezKHxH/LSIWR8SZZMMA/N1UkngdMUg6PS+JI+k4slLHgzXGUDYOAX8J7ImIP635+aVimCb9hnvojO0DeeuV84GnJqqBBhzHdCuMQdLrgNuAyyPi4QbjeEP+N0neimgOUOeHSmEMEbE0Is7M88PfAL8zU5I40JpWK68hm3hib/71tHz/GcCWHudfSP2tVgpjIHsj/y/ATrJS+LVN/CzI/hc68jh25MuaQf8+gFuAg2Qv/MaAD9Xw7DVkrXAeIRuBE+BK4Mp8XWQTmTwC3A+smKa/yaI4Xpt/z08DP8rXTxpwDDcCT3b8DYw29LO4Btidx/BN4O2DjqHr3JuYYa1W3EXfzKzl2lK1YmZmk3AiNzNrOSdyM7OWcyI3M2s5J3Izs5ZzIjczazkncjOzlvv/Q9gOp9ByNcgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final reward:  500\n"
     ]
    }
   ],
   "source": [
    "neural_network2 = PPONet_Policy()\n",
    "model_pol = neural_network2.load_model('Cartpole_weights/pol_it_74_eps_01_v2.h5')\n",
    "plot_policy(model_pol)\n",
    "\n",
    "run_game(model_pol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
